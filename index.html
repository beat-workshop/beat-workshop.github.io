<!doctype html><html lang=en><head><meta name=generator content="Hugo 0.147.8"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Behavior and Emotion Analysis through wearable Technology (BEAT)</title><meta name=description content="Workshop at IEEE International Conference on Automatic Face and Gesture Recognition 2026, Kyoto, Japan"><meta name=keywords content="workshop,FG2026,Kyoto,wearables"><meta name=referrer content="no-referrer-when-downgrade"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=/css/style.min.426d31a063d4922becd3481d6dc7c3146f9cee87184ca6028a4dc69b0abbe22c.css type=text/css integrity="sha256-Qm0xoGPUkivs00gdbcfDFG+c7ocYTKYCik3Gmwq74iw="><meta property="og:url" content="https://beat-workshop.github.io/"><meta property="og:site_name" content="Behavior and Emotion Analysis through wearable Technology (BEAT)"><meta property="og:title" content="Behavior and Emotion Analysis through wearable Technology (BEAT)"><meta property="og:description" content="Workshop at IEEE International Conference on Automatic Face and Gesture Recognition 2026, Kyoto, Japan"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Behavior and Emotion Analysis through wearable Technology (BEAT)"><meta name=twitter:description content="Workshop at IEEE International Conference on Automatic Face and Gesture Recognition 2026, Kyoto, Japan"><meta itemprop=name content="Behavior and Emotion Analysis through wearable Technology (BEAT)"><meta itemprop=description content="Workshop at IEEE International Conference on Automatic Face and Gesture Recognition 2026, Kyoto, Japan"></head><body><style>#site-head.withCenteredImage{background-image:url(/images/cover_hu_50b6abaaa65d1fec.webp)}@media(max-width:1920px) and (max-height:1280px){#site-head.withCenteredImage{background-image:url(/images/cover_hu_a7d33ac2ecdd96ad.webp)}}@media(max-width:1600px) and (max-height:1067px){#site-head.withCenteredImage{background-image:url(/images/cover_hu_a4fbc7b7bffc20d2.webp)}}@media(max-width:1366px) and (max-height:911px){#site-head.withCenteredImage{background-image:url(/images/cover_hu_4ca48bb198a716b0.webp)}}@media(max-height:1024px) and (max-aspect-ratio:900 / 1024){#site-head.withCenteredImage{background-image:url(/images/cover_hu_96ea93678f62df3e.webp)}}@media(max-height:1024px) and (max-aspect-ratio:600 / 1024){#site-head.withCenteredImage{background-image:url(/images/cover_hu_9a1894cfc0712f1a.webp)}}@media(max-height:1024px) and (max-aspect-ratio:360 / 1024){#site-head.withCenteredImage{background-image:url(/images/cover_hu_d84c231ef2fee86b.webp)}}</style><header id=site-head class=withCenteredImage><div class=vertical><div id=site-head-content class=inner><div class=title-and-description-guard><h1 class=blog-title>Behavior and Emotion Analysis through wearable Technology</h1><h2 class=blog-description>BEAT</h2></div><a class='btn site-menu' data-title-anchor=keynotes href=#keynotes>Keynotes</a>
<a class='btn site-menu' data-title-anchor=organizers href=#organizers>Organizers</a>
<a id=header-arrow href=#beat aria-label="Go to first section"><i class="fa fa-angle-down"></i></a></div></div></header><main class=content role=main><div class=fixed-nav><a class=fn-item item_index=1 href=#beat>BEAT</a>
<a class=fn-item item_index=2 href=#keynotes>Keynotes</a>
<a class=fn-item item_index=3 href=#organizers>Organizers</a>
<a class=fn-item item_index=4 href=./#site-head>Return To Top</a></div><div class=post-holder><article id=beat class='post first'><header class=post-header><h2 class=post-title>Description</h2></header><section class=post-content><p><strong>Wearable movement and physiology sensors</strong> such as smartwatches, smart
glasses, and ear-buds offer lightweight, non-invasive, and ecologically valid
means to monitor human activity, affective state, and social behavior. With the
rise of commercially deployed devices and new wearable foundation models,
opportunities for scalable human behavior analysis continue to grow. However,
challenges such as <strong>personalized modeling, on-device integration,</strong> or
<strong>multimodal fusion</strong> prevail, limiting in-the-wild deployment of wearable devices.</p><p>Recent research has focused on advancing both modeling and sensing
capabilities of wearable systems. In particular, the emergence of wearable
foundation models enables researchers to work with expressive representations
that scale to large populations and capture complex signals. On the other hand,
researchers are developing increasingly sophisticated devices to capture
physiological signals non-invasively, i.e., remote photoplethysmography (rPPG)
which can measure heart rate from videos of the face. These advances have
enabled the widespread adoption of commercial wearables such as
smartwatches, allowing individuals to monitor their sleep, mood, or health. They
are also supporting a variety of applications, from tracking affective states and
social behaviors, to support human-robot interaction, mobile health, and
behavioral research in both laboratory and real-world contexts.</p><p>However, significant challenges remain in both computational modeling and
sensor design. Human behavior is inherently complex, context-dependent, and
individual, making its analysis through wearable sensing particularly challenging.
Even in controlled environments where task complexity is limited, learning
personalized and/or generalized models remains difficult due to the high
variability across individuals and our incomplete understanding of the underlying
physiological mechanisms. This challenge is directly related to the multimodal
nature of human behaviors, with different physiological or movement signals
conveying distinct yet complementary information. Finally, as wearables are
designed to monitor people in real-world, uncontrolled settings, they bring
additional concerns related to privacy, ethical use, and data integrity.</p><p>The 1st Workshop on Behavioral and Emotion Analysis through wearable Technology (BEAT) aims to foster collaboration between ML researchers from various backgrounds (Gesture & Face Analysis, Affective Computing, HRI), as well as researchers in biomedical engineering. The main focus is on lightweight wearable movement and physiological sensors for computational human behavior analysis. While contributions are expected to be centered on real-world and ecologically valid settings, we also welcome controlled laboratory studies that introduce novel sensing approaches, benchmark datasets, or innovative applications.</p><p>Topics of interest include, but are not limited to:</p><ul><li>Machine Learning and computational models for movement and physiological wearables</li><li>Resource efficient and lightweight models</li><li>Multimodal fusion and synchronization strategies</li><li>Methods for irregularly sampled or missing data</li><li>Individual differences, personalization and context-awareness</li><li>Ethical and privacy-preserving AI in wearable systems</li><li>Novel wearables and applications</li><li>Experimental methods for validation of wearable systems</li><li>Lab-controlled experiments and In-the-wild deployment</li><li>Datasets and Benchmarks</li><li>Responsible data management and user consent</li><li>Applications in Affective Computing / Mobile Health / Action Recognition / Social Interaction / HRI</li></ul></section></article><div class='post-after light'></div></div><div class='post-holder dark'><article id=keynotes class=post><header class=post-header><h2 class=post-title>Keynotes</h2></header><section class=post-content><figure class=left><img src=/images/aaqib.webp alt="Aaqib Saeed" width=200></figure><p><strong>Aaqib Saeed</strong>, Eindhoven University of Technology
(TU/e), the Netherlands, is an Assistant Professor
(tenured) at Eindhoven University of Technology (TU/e).
He holds a Ph.D. cum laude from TU/e, where he
conducted research in the Department of Mathematics
and Computer Science, on self-supervised learning for
8
sensory data (ECG, EEG, IMU, PPG and Audio). Aaqib Saeed studies Artificial
Intelligence at the intersection of Self-Supervised Learning, Sensing Systems,
and Decentralized Computing. His work explores intelligent systems that can
learn from raw sensory signals (ECG, EEG, Audio, Speech, IMU) without
extensive human supervision, enabling scalable AI solutions for healthcare and
beyond.</p><figure class=clear><img src></figure><figure class=right><img src=/images/zilu.webp alt="Zilu Liang" width=200></figure><p><strong>Zilu Liang</strong>, Kyoto University of Advanced Science
(KUAS), Japan, is an Associate Professor at KUAS, where
she leads the Ubiquitous and Personal Computing Lab.
She received her Ph.D. and M.Sc. in Electrical
Engineering and Information Systems from the
University of Tokyo. Before joining KUAS, she held
research and academic positions at the University of
Tokyo, the University of Melbourne, the University of
Oxford, and Imperial College London. Her research
focuses on human-centered computing, wearable and ubiquitous technologies,
and AI-driven methods for understanding and supporting human behavior.</p><figure class=clear><img src></figure><figure class=left><img src=/images/kai.webp alt="Kai Kunze" width=200></figure><p><strong>Kai Kunze</strong>, Keio University (KMD), Japan, works as
Professor at the Graduate School of Media Design, Keio
University, Yokohama, Japan. Beforehand, he held an
Assistant Professorship at Osaka Prefecture University.
With over 254 papers published at high profile
conferences and journals (e.g. CHI, TOCHI, UIST, IEEE
Computer), Kai Kunze is a pioneer researcher in the
HCI field, augmenting humans using technology. His
most significant research contributions are in Eyewear
Computing and Placement Robust Activity Recognition. His current research also
includes Digitalizing Human Emotions and Amplifying Human Senses.</p><figure class=clear><img src></figure></section></article><div class=post-after></div></div><div class=post-holder><article id=organizers class='post last'><header class=post-header><h2 class=post-title>Organizers</h2></header><section class=post-content><p><figure class=left><img src=/images/louis.webp alt="Louis Simon" width=200></figure><strong>Louis Simon</strong></p><p>ISIR Laboratory, Sorbonne University (Paris, France).<figure class=clear><img src></figure></p><p><figure class=right><img src=/images/arianna.webp alt="Arianna De Vecchi" width=200></figure><strong>Arianna De Vecchi</strong></p><p>Politecnico di Milano and EssilorLuxottica (Milan, Italy).<figure class=clear><img src></figure></p><p><figure class=left><img src=/images/christina.webp alt="Cristina Palmero" width=200></figure><strong>Cristina Palmero</strong></p><p>Royal Academy of Engineering at Kingâ€™s College (London, UK).<figure class=clear><img src></figure></p><p><figure class=right><img src=/images/felix.webp alt="Felix Dollack" width=200></figure><strong>Felix Dollack</strong></p><p>Nara Institute of Science and Technology (Nara, Japan).<figure class=clear><img src></figure></p><p><figure class=left><img src=/images/ting.webp alt="Ting Dang" width=200></figure><strong>Ting Dang</strong></p><p>University of Melbourne, Australia.<figure class=clear><img src></figure></p><p><figure class=right><img src=/images/mohamed.webp alt="Mohamed Chetouani" width=200></figure><strong>Mohamed Chetouani</strong></p><p>Institute of Intelligent Systems and Robotics (CNRS UMR 7222), Sorbonne University (formerly Pierre and Marie Curie University).<figure class=clear><img src></figure></p></section></article><div class='post-after light'></div></div></main><footer class=site-footer><div class=inner><section class=links><ol></ol></section><hr><section class=icons></section></div></footer><script src=/js/script.min.e853a4dfd899ef0f5749c33d64e2388119f4f8f866221b83ae75f840abec921a.js integrity="sha256-6FOk39iZ7w9XScM9ZOI4gRn0+PhmIhuDrnX4QKvskho="></script><script>cssVars()</script></body></html>