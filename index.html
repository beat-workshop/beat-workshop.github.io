<!doctype html><html lang=en><head><meta name=generator content="Hugo 0.147.8"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Behavior and Emotion Analysis through wearable Technology (BEAT)</title><link rel="shortcut icon" href=/images/beat.webp type=image/webp><meta name=description content="Workshop at IEEE International Conference on Automatic Face and Gesture Recognition 2026, Kyoto, Japan"><meta name=keywords content="workshop,FG2026,Kyoto,wearables"><meta name=referrer content="no-referrer-when-downgrade"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=/css/style.min.426d31a063d4922becd3481d6dc7c3146f9cee87184ca6028a4dc69b0abbe22c.css type=text/css integrity="sha256-Qm0xoGPUkivs00gdbcfDFG+c7ocYTKYCik3Gmwq74iw="><meta property="og:url" content="https://beat-workshop.github.io/"><meta property="og:site_name" content="Behavior and Emotion Analysis through wearable Technology (BEAT)"><meta property="og:title" content="Behavior and Emotion Analysis through wearable Technology (BEAT)"><meta property="og:description" content="Workshop at IEEE International Conference on Automatic Face and Gesture Recognition 2026, Kyoto, Japan"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Behavior and Emotion Analysis through wearable Technology (BEAT)"><meta name=twitter:description content="Workshop at IEEE International Conference on Automatic Face and Gesture Recognition 2026, Kyoto, Japan"><meta itemprop=name content="Behavior and Emotion Analysis through wearable Technology (BEAT)"><meta itemprop=description content="Workshop at IEEE International Conference on Automatic Face and Gesture Recognition 2026, Kyoto, Japan"></head><body><style>#site-head.withCenteredImage{background-image:url(/images/cover_hu_50b6abaaa65d1fec.webp)}@media(max-width:1920px) and (max-height:1280px){#site-head.withCenteredImage{background-image:url(/images/cover_hu_a7d33ac2ecdd96ad.webp)}}@media(max-width:1600px) and (max-height:1067px){#site-head.withCenteredImage{background-image:url(/images/cover_hu_a4fbc7b7bffc20d2.webp)}}@media(max-width:1366px) and (max-height:911px){#site-head.withCenteredImage{background-image:url(/images/cover_hu_4ca48bb198a716b0.webp)}}@media(max-height:1024px) and (max-aspect-ratio:900 / 1024){#site-head.withCenteredImage{background-image:url(/images/cover_hu_96ea93678f62df3e.webp)}}@media(max-height:1024px) and (max-aspect-ratio:600 / 1024){#site-head.withCenteredImage{background-image:url(/images/cover_hu_9a1894cfc0712f1a.webp)}}@media(max-height:1024px) and (max-aspect-ratio:360 / 1024){#site-head.withCenteredImage{background-image:url(/images/cover_hu_d84c231ef2fee86b.webp)}}</style><header id=site-head class=withCenteredImage><div class=vertical><div id=site-head-content class=inner><div class=title-and-description-guard><h1 class=blog-title>Behavior and Emotion Analysis through wearable Technology</h1><h2 class=blog-description>BEAT</h2></div><a class='btn site-menu' data-title-anchor=keynotes href=#keynotes>Keynotes</a>
<a class='btn site-menu' data-title-anchor=important-dates href=#important-dates>Important Dates</a>
<a class='btn site-menu' data-title-anchor=organizers href=#organizers>Organizers</a>
<a id=header-arrow href=#beat aria-label="Go to first section"><i class="fa fa-angle-down"></i></a></div></div></header><main class=content role=main><div class=fixed-nav><a class=fn-item item_index=1 href=#beat>BEAT</a>
<a class=fn-item item_index=2 href=#keynotes>Keynotes</a>
<a class=fn-item item_index=3 href=#call-for-papers>Call for Papers</a>
<a class=fn-item item_index=4 href=#important-dates>Important Dates</a>
<a class=fn-item item_index=5 href=#organizers>Organizers</a>
<a class=fn-item item_index=6 href=./#site-head>Return To Top</a></div><div class=post-holder><article id=beat class='post first'><header class=post-header><h2 class=post-title>Workshop Description</h2></header><section class=post-content><p><strong>Wearable movement and physiology sensors</strong> such as smartwatches, smart
glasses, and ear-buds offer lightweight, non-invasive, and ecologically valid
means to monitor human activity, affective state, and social behavior. With the
rise of commercially deployed devices and new wearable foundation models,
opportunities for scalable human behavior analysis continue to grow. However,
challenges such as <strong>personalized modeling, on-device integration,</strong> or
<strong>multimodal fusion</strong> prevail, limiting in-the-wild deployment of wearable devices.</p><p>The 1st Workshop on Behavioral and Emotion Analysis through wearable Technology (BEAT) aims to foster collaboration between ML researchers from various backgrounds (Gesture & Face Analysis, Affective Computing, HRI), as well as researchers in biomedical engineering. The main focus is on lightweight wearable movement and physiological sensors for computational human behavior analysis. While contributions are expected to be centered on real-world and ecologically valid settings, we also welcome controlled laboratory studies that introduce novel sensing approaches, benchmark datasets, or innovative applications.</p><p>Participation in this workshop will take place in person during the <a href=https://fg2026.ieee-biometrics.org>IEEE FG 2026</a>, held in Kyoto from 25-29 May 2026.</p></section></article><div class='post-after light'></div></div><div class='post-holder dark'><article id=keynotes class=post><header class=post-header><h2 class=post-title>Keynotes</h2></header><section class=post-content><figure class=left><img src=/images/aaqib.webp alt="Aaqib Saeed" width=200></figure><p><strong>Aaqib Saeed</strong>, Eindhoven University of Technology
(TU/e), the Netherlands, is an Assistant Professor
(tenured) at Eindhoven University of Technology (TU/e).
He holds a Ph.D. cum laude from TU/e, where he
conducted research in the Department of Mathematics
and Computer Science, on self-supervised learning for
8
sensory data (ECG, EEG, IMU, PPG and Audio). Aaqib Saeed studies Artificial
Intelligence at the intersection of Self-Supervised Learning, Sensing Systems,
and Decentralized Computing. His work explores intelligent systems that can
learn from raw sensory signals (ECG, EEG, Audio, Speech, IMU) without
extensive human supervision, enabling scalable AI solutions for healthcare and
beyond.</p><figure class=clear><img src></figure><figure class=right><img src=/images/zilu.webp alt="Zilu Liang" width=200></figure><p><strong>Zilu Liang</strong>, Kyoto University of Advanced Science
(KUAS), Japan, is an Associate Professor at KUAS, where
she leads the Ubiquitous and Personal Computing Lab.
She received her Ph.D. and M.Sc. in Electrical
Engineering and Information Systems from the
University of Tokyo. Before joining KUAS, she held
research and academic positions at the University of
Tokyo, the University of Melbourne, the University of
Oxford, and Imperial College London. Her research
focuses on human-centered computing, wearable and ubiquitous technologies,
and AI-driven methods for understanding and supporting human behavior.</p><figure class=clear><img src></figure><figure class=left><img src=/images/kai.webp alt="Kai Kunze" width=200></figure><p><strong>Kai Kunze</strong>, Keio University (KMD), Japan, works as
Professor at the Graduate School of Media Design, Keio
University, Yokohama, Japan. Beforehand, he held an
Assistant Professorship at Osaka Prefecture University.
With over 254 papers published at high profile
conferences and journals (e.g. CHI, TOCHI, UIST, IEEE
Computer), Kai Kunze is a pioneer researcher in the
HCI field, augmenting humans using technology. His
most significant research contributions are in Eyewear
Computing and Placement Robust Activity Recognition. His current research also
includes Digitalizing Human Emotions and Amplifying Human Senses.</p><figure class=clear><img src></figure></section></article><div class=post-after></div></div><div class=post-holder><article id=call-for-papers class=post><header class=post-header><h2 class=post-title>Call for Papers</h2></header><section class=post-content><p>We welcome submissions to two tracks:</p><ul><li><p><strong>Main Track (Original Research):</strong> This track accepts original and unpublished work of 4 to 8-pages (excluding references). All submissions will undergo a double-blind review process, with three reviewers assigned to each paper. Accepted papers will be included in the workshop proceedings, and presented as posters or oral presentations.</p></li><li><p><strong>Non-Archival Track (Published Work):</strong> Authors of papers previously published in other conferences or journals are invited to submit a 1-page summary to present their work, which will not be included in the proceedings. These submissions will be evaluated and selected by the organizers based on quality and relevance to the workshop topics.</p></li></ul><p>Topics of interest include, but are not limited to:</p><ul><li>Machine Learning and computational models for movement and physiological wearables</li><li>Resource efficient and lightweight models</li><li>Multimodal fusion and synchronization strategies</li><li>Methods for irregularly sampled or missing data</li><li>Individual differences, personalization and context-awareness</li><li>Ethical and privacy-preserving AI in wearable systems</li><li>Novel wearables and applications</li><li>Experimental methods for validation of wearable systems</li><li>Lab-controlled experiments and In-the-wild deployment</li><li>Datasets and Benchmarks</li><li>Responsible data management and user consent</li><li>Applications in Affective Computing / Mobile Health / Action Recognition / Social Interaction / HRI</li></ul></section></article><div class='post-after light'></div></div><div class='post-holder dark'><article id=important-dates class=post><header class=post-header><h2 class=post-title>Important Dates</h2></header><section class=post-content><table><thead><tr><th style=text-align:left>Main track</th><th style=text-align:right></th><th style=text-align:right></th><th></th></tr></thead><tbody><tr><td style=text-align:left>Paper registration deadline</td><td style=text-align:right>March</td><td style=text-align:right>30</td><td style=text-align:left>2026</td></tr><tr><td style=text-align:left>Paper submission deadline</td><td style=text-align:right>April</td><td style=text-align:right>3</td><td style=text-align:left>2026</td></tr><tr><td style=text-align:left>Notification to authors</td><td style=text-align:right>April</td><td style=text-align:right>15</td><td style=text-align:left>2026</td></tr><tr><td style=text-align:left>Camera-ready paper deadline</td><td style=text-align:right>April</td><td style=text-align:right>21</td><td style=text-align:left>2026</td></tr></tbody></table><table><thead><tr><th style=text-align:left>Non-archival track</th><th style=text-align:right></th><th style=text-align:right></th><th></th></tr></thead><tbody><tr><td style=text-align:left>Summary submission deadline</td><td style=text-align:right>April</td><td style=text-align:right>3</td><td style=text-align:left>2026</td></tr><tr><td style=text-align:left>Notification to authors</td><td style=text-align:right>April</td><td style=text-align:right>15</td><td style=text-align:left>2026</td></tr></tbody></table></section></article><div class=post-after></div></div><div class=post-holder><article id=organizers class='post last'><header class=post-header><h2 class=post-title>Organizers</h2></header><section class=post-content><p><figure class=left><img src=/images/louis.webp alt="Louis Simon" width=200></figure><strong>Louis Simon</strong></p><p>Sorbonne University, France.<figure class=clear><img src></figure></p><p><figure class=right><img src=/images/arianna.webp alt="Arianna De Vecchi" width=200></figure><strong>Arianna De Vecchi</strong></p><p>Politecnico di Milano and EssilorLuxottica, Italy.<figure class=clear><img src></figure></p><p><figure class=left><img src=/images/christina.webp alt="Cristina Palmero" width=200></figure><strong>Cristina Palmero</strong></p><p>Kingâ€™s College London, UK.<figure class=clear><img src></figure></p><p><figure class=right><img src=/images/felix.webp alt="Felix Dollack" width=200></figure><strong>Felix Dollack</strong></p><p>Nara Institute of Science and Technology, Japan.<figure class=clear><img src></figure></p><p><figure class=left><img src=/images/ting.webp alt="Ting Dang" width=200></figure><strong>Ting Dang</strong></p><p>University of Melbourne, Australia.<figure class=clear><img src></figure></p><p><figure class=right><img src=/images/mohamed.webp alt="Mohamed Chetouani" width=200></figure><strong>Mohamed Chetouani</strong></p><p>Sorbonne University, France.<figure class=clear><img src></figure></p></section></article><div class='post-after light'></div></div></main><footer class=site-footer><div class=inner><section class=links><ol><li><a href=/credits/>Credits</a></li><li><a href=/license/>License</a></li></ol></section><hr><section class=icons></section></div></footer><script src=/js/script.min.e853a4dfd899ef0f5749c33d64e2388119f4f8f866221b83ae75f840abec921a.js integrity="sha256-6FOk39iZ7w9XScM9ZOI4gRn0+PhmIhuDrnX4QKvskho="></script><script>cssVars()</script></body></html>